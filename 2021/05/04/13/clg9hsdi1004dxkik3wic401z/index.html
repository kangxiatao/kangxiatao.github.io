<!doctype html>
<html lang="zh"><script src="/js/click.js"></script><script src="https://code.jquery.com/jquery-3.0.0.min.js"></script><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="baidu-site-verification" content="codeva-PPGKUZV4wV"><meta name="google-site-verification" content="ij3WihX2SeS5cGe8UJ-rf66Ru7F9z_zHFG86nbIASa0"><meta><title>神经网络剪枝类论文阅读笔记 - AnoI</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="AnoI"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="AnoI"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="后面看的论文都懒得更新了限于实验条件勉强做了两年剪枝，不过也就剪枝能让我有机会卷顶会了，挺好。 终于到点了，我还算努力，只是后来知道了水论文的无趣便没怎么看文献。 这段时间写毕业论文全靠自己的文档和博客，好好感谢自己吧。只希望以后我能多写博客吧，别搁这无所谓无所谓。 算了确实无所谓。 0、my codeshttps:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;kangxiatao&amp;#x2F;prune-masterhttps"><meta property="og:type" content="blog"><meta property="og:title" content="神经网络剪枝类论文阅读笔记"><meta property="og:url" content="http://example.com/2021/05/04/13/clg9hsdi1004dxkik3wic401z/"><meta property="og:site_name" content="AnoI"><meta property="og:description" content="后面看的论文都懒得更新了限于实验条件勉强做了两年剪枝，不过也就剪枝能让我有机会卷顶会了，挺好。 终于到点了，我还算努力，只是后来知道了水论文的无趣便没怎么看文献。 这段时间写毕业论文全靠自己的文档和博客，好好感谢自己吧。只希望以后我能多写博客吧，别搁这无所谓无所谓。 算了确实无所谓。 0、my codeshttps:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;kangxiatao&amp;#x2F;prune-masterhttps"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:published_time" content="2021-05-04T05:29:08.000Z"><meta property="article:modified_time" content="2023-03-23T11:29:28.261Z"><meta property="article:author" content="Kang Xiatao"><meta property="article:tag" content="神经网络"><meta property="article:tag" content="神经网络剪枝"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2021/05/04/13/clg9hsdi1004dxkik3wic401z/"},"headline":"神经网络剪枝类论文阅读笔记","image":["http://example.com/img/og_image.png"],"datePublished":"2021-05-04T05:29:08.000Z","dateModified":"2023-03-23T11:29:28.261Z","author":{"@type":"Person","name":"Kang Xiatao"},"publisher":{"@type":"Organization","name":"AnoI","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"后面看的论文都懒得更新了限于实验条件勉强做了两年剪枝，不过也就剪枝能让我有机会卷顶会了，挺好。 终于到点了，我还算努力，只是后来知道了水论文的无趣便没怎么看文献。 这段时间写毕业论文全靠自己的文档和博客，好好感谢自己吧。只希望以后我能多写博客吧，别搁这无所谓无所谓。 算了确实无所谓。 0、my codeshttps:&#x2F;&#x2F;github.com&#x2F;kangxiatao&#x2F;prune-masterhttps"}</script><link rel="canonical" href="http://example.com/2021/05/04/13/clg9hsdi1004dxkik3wic401z/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><script src="/js/cursor2.js"></script><script type="text/javascript">$.shuicheMouse();</script><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="AnoI" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">类别</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="To GitHub" href="https://github.com/kangxiatao"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-05-04T05:29:08.000Z" title="2021/5/4 13:29:08">2021-05-04</time>发表</span><span class="level-item"><time dateTime="2023-03-23T11:29:28.261Z" title="2023/3/23 19:29:28">2023-03-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/">学习日志</a></span></div></div><h1 class="title is-3 is-size-4-mobile">神经网络剪枝类论文阅读笔记</h1><div class="content"><h2 id="后面看的论文都懒得更新了"><a href="#后面看的论文都懒得更新了" class="headerlink" title="后面看的论文都懒得更新了"></a>后面看的论文都懒得更新了</h2><p>限于实验条件勉强做了两年剪枝，不过也就剪枝能让我有机会卷顶会了，挺好。</p>
<p>终于到点了，我还算努力，只是后来知道了水论文的无趣便没怎么看文献。</p>
<p>这段时间写毕业论文全靠自己的文档和博客，好好感谢自己吧。只希望以后我能多写博客吧，别搁这无所谓无所谓。</p>
<p>算了确实无所谓。</p>
<h2 id="0、my-codes"><a href="#0、my-codes" class="headerlink" title="0、my codes"></a>0、my codes</h2><p><a target="_blank" rel="noopener" href="https://github.com/kangxiatao/prune-master">https://github.com/kangxiatao/prune-master</a><br><a target="_blank" rel="noopener" href="https://gitee.com/kang-xiatao/prune-master">https://gitee.com/kangxiatao/prune-master</a></p>
<span id="more"></span>

<h2 id="1、筛选修剪"><a href="#1、筛选修剪" class="headerlink" title="1、筛选修剪"></a>1、筛选修剪</h2><h3 id="AutoPrune-Automatic-Network-Pruning-by-Regularizing-Auxiliary-Parameters"><a href="#AutoPrune-Automatic-Network-Pruning-by-Regularizing-Auxiliary-Parameters" class="headerlink" title="AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters"></a>AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters</h3><p>自动剪枝：通过正则化辅助参数自动进行网络剪枝</p>
<p>作者提出AutoPrune，通过优化一组辅助参数来实现剪枝，不过是基于权重剪枝，或能推广到神经元剪枝。</p>
<p>其实是用这组超参数把网络稀疏问题表述为一个优化问题，一组超参数与权重的乘积表示为修剪后的权值矩阵。</p>
<p>在我看来有个很明显的缺点，不管结果如何，用测试集得到一种先验就很蠢，他是根据测试集L1正则化得到的M和训练集L1正则化得到的W，M为所说的辅助参数，M x W为新的权重。</p>
<h3 id="A-dynamic-CNN-pruning-method-based-on-matrix-similarity"><a href="#A-dynamic-CNN-pruning-method-based-on-matrix-similarity" class="headerlink" title="A dynamic CNN pruning method based on matrix similarity"></a>A dynamic CNN pruning method based on matrix similarity</h3><p>一种基于矩阵相似度的动态CNN剪枝方法</p>
<p>因与我的论文有一点相似之处，所以纳入进来。</p>
<p>大概思路：选过滤器或特征图进行矩阵相似度对比，按一定比例删除相似的。</p>
<h3 id="Channel-Pruning-for-Accelerating-Very-Deep-Neural-Networks"><a href="#Channel-Pruning-for-Accelerating-Very-Deep-Neural-Networks" class="headerlink" title="Channel Pruning for Accelerating Very Deep Neural Networks"></a>Channel Pruning for Accelerating Very Deep Neural Networks</h3><p>通道剪枝加速神经网络</p>
<p>这是一篇ICCV2017的文章，关于用通道剪枝（channel pruning）来做模型加速，通道减枝是模型压缩和加速领域的一个重要分支。</p>
<p>作者不是传统的按范数值剪枝，他利用通道间的冗余来剪枝，先用lasso找出具有代表性的通道，再用最小二乘法重构剩余通道的输出。</p>
<p>最开始的权重是无惩罚训练的，lasso惩罚用来找一个掩码向量（选取通道），最小二次法再进一步优化，个人不太喜欢这种做法，直接对权重做lasso惩罚，虽然没有显式的求解lasso，但简洁有效的做到了稀疏。</p>
<h3 id="Channel-level-Acceleration-of-Deep-Face-Representations"><a href="#Channel-level-Acceleration-of-Deep-Face-Representations" class="headerlink" title="Channel-level Acceleration of Deep Face Representations"></a>Channel-level Acceleration of Deep Face Representations</h3><p>深度人脸表征的通道级加速</p>
<p>目前的一些网络压缩不一定适用于深层人脸网络，作者提出两种方法，一种基于消除低活跃通道，另一种基于耦合剪枝，重复使用已经计算的元素。</p>
<p>感觉这篇论文有点水，中间没有细看。</p>
<h3 id="Dynamic-Channel-Pruning-Feature-Boosting-and-Suppression"><a href="#Dynamic-Channel-Pruning-Feature-Boosting-and-Suppression" class="headerlink" title="Dynamic Channel Pruning: Feature Boosting and Suppression"></a>Dynamic Channel Pruning: Feature Boosting and Suppression</h3><p>动态通道剪枝：特征增强和抑制</p>
<p>作者发现神经元的重要性高度依赖输入，提出了动态剪枝，动态模型保证了模型的完整性。在推理过程中，动态网络可以使用输入数据来选择网络的部分进行评估，于是乎可以做增强或抑制处理。</p>
<p>不过动态的剪枝也只能是在运算的环节起到加速作用，并没有减少权重的存储量。</p>
<h3 id="Dynamic-Network-Surgery-for-Efficient-DNNs"><a href="#Dynamic-Network-Surgery-for-Efficient-DNNs" class="headerlink" title="Dynamic Network Surgery for Efficient DNNs"></a>Dynamic Network Surgery for Efficient DNNs</h3><p>高效DNN的动态网络手术</p>
<p>作者用pruning 和 splicing 来实现动态剪枝，与上一篇不同，此文的动态是指恢复一些被剪掉的连接，因为在复杂的神经网络连接中，连接的重要性难以衡量，可能存在其他冗余的连接被剪掉之后，它可能会变得极其重要。</p>
<p>实现方案是用一个掩码矩阵来标记，通过判别函数得到掩码矩阵。判别函数通过权重的绝对值平方和方差与设定的上下两个阈值比较得到。</p>
<p>我觉得不行。</p>
<h3 id="Efficient-Hardware-Realization-of-Convolutional-Neural-Networks-using-Intra-Kernel-Regular-Pruning"><a href="#Efficient-Hardware-Realization-of-Convolutional-Neural-Networks-using-Intra-Kernel-Regular-Pruning" class="headerlink" title="Efficient Hardware Realization of Convolutional Neural Networks using Intra-Kernel Regular Pruning"></a>Efficient Hardware Realization of Convolutional Neural Networks using Intra-Kernel Regular Pruning</h3><p>基于核内规则剪枝的卷积神经网络的高效硬件实现</p>
<p>针对细颗粒修剪，考虑多种修剪情况，选取修剪后最好的一种模式。</p>
<h3 id="Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration"><a href="#Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration" class="headerlink" title="Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration"></a>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration</h3><p>用于深度卷积神经网络加速的几何中值滤波剪枝</p>
<p>作者提出用范数值大小剪枝不合理，按中值剪枝合理。</p>
<h3 id="Infinite-Feature-Selection"><a href="#Infinite-Feature-Selection" class="headerlink" title="Infinite Feature Selection"></a>Infinite Feature Selection</h3><p>无限的特征选择</p>
<p>本文提出的是一个滤波算法，评估特征的重要性来选择特征，感觉是纯学术上瞎胡闹，不看了。</p>
<h3 id="Learning-to-Prune-Filters-in-Convolutional-Neural-Networks"><a href="#Learning-to-Prune-Filters-in-Convolutional-Neural-Networks" class="headerlink" title="Learning to Prune Filters in Convolutional Neural Networks"></a>Learning to Prune Filters in Convolutional Neural Networks</h3><p>学习修剪神经网络的过滤器</p>
<p>文章提出了一种“try-and-learn”的算法去训练一个pruning agent，过程也很容易理解，先随机剪掉一些，再根据评估得到奖励，利用奖励机制优化pruning agent。</p>
<p>这篇文章和我们的无损剪枝有点像，倒也用不着这么复杂。</p>
<h3 id="NISP-Pruning-Networks-using-Neuron-Importance-Score-Propagation"><a href="#NISP-Pruning-Networks-using-Neuron-Importance-Score-Propagation" class="headerlink" title="NISP: Pruning Networks using Neuron Importance Score Propagation"></a>NISP: Pruning Networks using Neuron Importance Score Propagation</h3><p>利用神经元重要性评分传播网络剪枝</p>
<p>作者发现目前的剪枝方法考虑的只是单层或相邻的两层，忽略了网络中整体传播的影响，思路是用一个神经网络重要性分数传播(NISP)算法，将网络剪枝描述为一个二进制整数优化问题。</p>
<p>实现部分我懒得看了，差不多得了。</p>
<h3 id="Optimal-Brain-Damage"><a href="#Optimal-Brain-Damage" class="headerlink" title="Optimal Brain Damage"></a>Optimal Brain Damage</h3><p>最佳脑损伤</p>
<p>这篇文章最让我震惊的是它的发表自1990，至今在各种网络中都能取得非常优异剪枝效果。我在最新的一些论文中总是能看到它的影子，具体做法就是计算目标函数对参数求二阶导数，用于表示参数的贡献度，基于贡献度修剪，其中的公式和近似方法先保留，很经典。</p>
<h3 id="Optimal-Brain-Surgeon-and-General-Network-Prunng"><a href="#Optimal-Brain-Surgeon-and-General-Network-Prunng" class="headerlink" title="Optimal Brain Surgeon and General Network Prunng"></a>Optimal Brain Surgeon and General Network Prunng</h3><p>最佳脑外科医生和一般网络修剪</p>
<p>OBD的改进版，主要运算方面的改进，递归计算海森矩阵的逆矩阵。</p>
<h3 id="Pruning-Filter-in-Filter"><a href="#Pruning-Filter-in-Filter" class="headerlink" title="Pruning Filter in Filter"></a>Pruning Filter in Filter</h3><p>在过滤器中修剪过滤器</p>
<p>作者认为修剪后的结构比权重更为重要，并引入一个可学习矩阵作为滤波器骨架，反应每个滤波器的形状。</p>
<h3 id="Rethinking-the-Smaller-Norm-Less-Informative-Assumption-in-Channel-Pruning-of-Convolution-Layers"><a href="#Rethinking-the-Smaller-Norm-Less-Informative-Assumption-in-Channel-Pruning-of-Convolution-Layers" class="headerlink" title="Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers"></a>Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers</h3><p>重新思考卷积层通道剪枝中的值小不重要假设</p>
<p>作者对依据范数值大小剪枝做了些讨论，并提出给卷积通道加一个门，来判断该通道有没有起作用。</p>
<h3 id="Strategies-for-Re-training-a-Pruned-Neural-Network-in-an-Edge-Computing-Paradigm"><a href="#Strategies-for-Re-training-a-Pruned-Neural-Network-in-an-Edge-Computing-Paradigm" class="headerlink" title="Strategies for Re-training a Pruned Neural Network in an Edge Computing Paradigm"></a>Strategies for Re-training a Pruned Neural Network in an Edge Computing Paradigm</h3><p>在边缘计算范式下重新训练修剪神经网络的策略</p>
<p>部署在边缘移动设备之后很难再重新训练，作者提出了剪枝后重训练的概念。</p>
<h3 id="Soft-filter-pruning-for-accelerating-deep-convolutional-neural-networks"><a href="#Soft-filter-pruning-for-accelerating-deep-convolutional-neural-networks" class="headerlink" title="Soft filter pruning for accelerating deep convolutional neural networks"></a>Soft filter pruning for accelerating deep convolutional neural networks</h3><p>用于加速深度卷积神经网络的软过滤器修剪</p>
<p>一种动态剪枝方法，它能够让被剪掉的过滤器参与一定的训练从而提高剪枝的效率。</p>
<h3 id="The-Generalization-Stability-Tradeoff-In-Neural-Network-Pruning"><a href="#The-Generalization-Stability-Tradeoff-In-Neural-Network-Pruning" class="headerlink" title="The Generalization-Stability Tradeoff In Neural Network Pruning"></a>The Generalization-Stability Tradeoff In Neural Network Pruning</h3><p>神经网络剪枝中的泛化-稳定性权衡</p>
<p>文章对泛化展开了一些讨论，最后通过自己的实验得到如下结论：“减少参数量，减少网络对噪声的过拟合，从而提高泛化能力（test acc）” 的说法是不够准确的，因为大网络在不减少参数量的情况下，一样可以有很好的泛化能力，剪枝能提高网络性能的根本原因是在训练过程中引入了训练噪声，从而提高了泛化能力。</p>
<h3 id="Parameter-Efficient-Training-of-Deep-Convolutional-Neural-Networks-by-Dynamic-Sparse-Reparameterization"><a href="#Parameter-Efficient-Training-of-Deep-Convolutional-Neural-Networks-by-Dynamic-Sparse-Reparameterization" class="headerlink" title="Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization"></a>Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization</h3><p>Hesham Mostafa and Xin Wang, Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization, Proc. Int. Conf. Machine Learning (ICML), 2019.</p>
<p>基于动态稀疏重参数化的深卷积神经网络参数有效训练</p>
<p>训练过程中会动态地改变网络的稀疏结构，取得了优异的性能，作者表明在有限存储和计算预算下训练一个CNN，分配部分资源来描述和演化网络的结构比完全花在一个稠密网络参数上更好。</p>
<h3 id="MLPrune-Multi-Layer-Pruning-for-Automated-Neural-Network-Compression"><a href="#MLPrune-Multi-Layer-Pruning-for-Automated-Neural-Network-Compression" class="headerlink" title="MLPrune: Multi-Layer Pruning for Automated Neural Network Compression"></a>MLPrune: Multi-Layer Pruning for Automated Neural Network Compression</h3><p>Wenyuan Zeng and Raquel Urtasun. MLPrune: Multi-layer pruning for automated neural network<br>compression, 2019. URL <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=r1g5b2RcKm">https://openreview.net/forum?id=r1g5b2RcKm</a>.</p>
<p>用于自动神经网络压缩的多层剪枝</p>
<p>基于 Kronecker 因子近似曲率方法，使用 Hessian 的有效近似作为我们的修剪标准。</p>
<h3 id="Exploiting-Kernel-Sparsity-and-Entropy-for-Interpretable-CNN-Compression"><a href="#Exploiting-Kernel-Sparsity-and-Entropy-for-Interpretable-CNN-Compression" class="headerlink" title="Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression"></a>Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression</h3><p>Li Y, Lin S, Zhang B, et al. Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression. 2018.</p>
<p>利用核稀疏性和熵实现可解释CNN压缩</p>
<p>基于KSE指标进一步进行核聚类，实现高精度CNN压缩.</p>
<p>需要硬件支持卷积核稀疏。</p>
<h3 id="Group-Fisher-Pruning-for-Practical-Network-Compression"><a href="#Group-Fisher-Pruning-for-Practical-Network-Compression" class="headerlink" title="Group Fisher Pruning for Practical Network Compression"></a>Group Fisher Pruning for Practical Network Compression</h3><p>ICML. 2021.</p>
<p>群Fisher修剪在实际网络压缩中的应用</p>
<p>一种分层分组算法，寻找耦合的通道，利用梯度计算的链式原则得到通道的重要性，迭代移除不重要的通道，最后再微调。</p>
<hr>
<p><strong>&#x3D;&gt; 理论研究：</strong></p>
<p>A Probabilistic Approach to Neural Network Pruning</p>
<p>一种神经网络剪枝的概率方法</p>
<p>提供一种通用的方法在概率意义上限制修剪后的网络和目标网络之间的间隙</p>
<hr>
<h2 id="2、稀疏惩罚"><a href="#2、稀疏惩罚" class="headerlink" title="2、稀疏惩罚"></a>2、稀疏惩罚</h2><h3 id="Fast-ConvNets-Using-Group-wise-Brain-Damage"><a href="#Fast-ConvNets-Using-Group-wise-Brain-Damage" class="headerlink" title="Fast ConvNets Using Group-wise Brain Damage"></a>Fast ConvNets Using Group-wise Brain Damage</h3><p>使用群体脑损伤的快速卷积网络</p>
<p>没有特别细致的看，不知道是不是最先提出的，以组的方式修剪卷积张量核。</p>
<h3 id="LEARNING-SPARSE-NEURAL-NETWORKS-THROUGH-L0-REGULARIZATION"><a href="#LEARNING-SPARSE-NEURAL-NETWORKS-THROUGH-L0-REGULARIZATION" class="headerlink" title="LEARNING SPARSE NEURAL NETWORKS THROUGH L0 REGULARIZATION"></a>LEARNING SPARSE NEURAL NETWORKS THROUGH L0 REGULARIZATION</h3><p>通过正则化L0学习稀疏神经网络</p>
<p>主要是提出一种分布来替代L0优化目标函数，类似伯努利分布的推广，但是我偷懒，没有推导下去。</p>
<h3 id="Learning-the-Number-of-Neurons-in-Deep-Networks"><a href="#Learning-the-Number-of-Neurons-in-Deep-Networks" class="headerlink" title="Learning the Number of Neurons in Deep Networks"></a>Learning the Number of Neurons in Deep Networks</h3><p>学习深度神经网络的神经元数量</p>
<p>使用一个稀疏正则化器，让网络变得更紧凑，文中对group lasso求了闭式解，表明最终神经元参数会趋零。</p>
<h3 id="Learning-Structured-Sparsity-in-Deep-Neural-Networks"><a href="#Learning-Structured-Sparsity-in-Deep-Neural-Networks" class="headerlink" title="Learning Structured Sparsity in Deep Neural Networks"></a>Learning Structured Sparsity in Deep Neural Networks</h3><p>深度神经网络的结构稀疏性学习</p>
<p>对Lasso的一种推广，即将特征分组后的Lasso，Group Lasso。</p>
<h3 id="Learning-Efficient-Convolutional-Networks-through-Network-Slimming"><a href="#Learning-Efficient-Convolutional-Networks-through-Network-Slimming" class="headerlink" title="Learning Efficient Convolutional Networks through Network Slimming"></a>Learning Efficient Convolutional Networks through Network Slimming</h3><p>通过网络瘦身学习高效卷积网络</p>
<p>将L1正则化施加到BN层的缩放因子上，L1正则化推动BN层的缩放因子趋向于零，从而鉴别出不重要的通道或者神经元。</p>
<h3 id="Pruning-Filters-for-Efficient-ConvNets"><a href="#Pruning-Filters-for-Efficient-ConvNets" class="headerlink" title="Pruning Filters for Efficient ConvNets"></a>Pruning Filters for Efficient ConvNets</h3><p>高效卷积网络的剪枝过滤器</p>
<p>采用L1范数来评估CNN中过滤器的重要性，实现模型剪枝。</p>
<h3 id="PRUNE-YOUR-NEURONS-BLINDLY-NEURAL-NETWORK-COMPRESSION-THROUGH-STRUCTURED-CLASS-BLIND-PRUNING"><a href="#PRUNE-YOUR-NEURONS-BLINDLY-NEURAL-NETWORK-COMPRESSION-THROUGH-STRUCTURED-CLASS-BLIND-PRUNING" class="headerlink" title="PRUNE YOUR NEURONS BLINDLY: NEURAL NETWORK COMPRESSION THROUGH STRUCTURED CLASS-BLIND PRUNING"></a>PRUNE YOUR NEURONS BLINDLY: NEURAL NETWORK COMPRESSION THROUGH STRUCTURED CLASS-BLIND PRUNING</h3><p>盲目地修剪你的神经元：神经网络通过结构化的类盲修剪进行压缩</p>
<p>标题很装啊，就是L1惩罚稀疏后结构化剪枝，使得网络更紧凑非稀疏。</p>
<h3 id="Sparse-Convolutional-Neural-Networks"><a href="#Sparse-Convolutional-Neural-Networks" class="headerlink" title="Sparse Convolutional Neural Networks"></a>Sparse Convolutional Neural Networks</h3><p>稀疏卷积神经网络</p>
<p>矩阵分解加稀疏约束的融合怪罢了</p>
<h3 id="TRAINING-COMPRESSED-FULLY-CONNECTED-NET-WORKS-WITH-A-DENSITY-DIVERSITY-PENALTY"><a href="#TRAINING-COMPRESSED-FULLY-CONNECTED-NET-WORKS-WITH-A-DENSITY-DIVERSITY-PENALTY" class="headerlink" title="TRAINING COMPRESSED FULLY-CONNECTED NET-WORKS WITH A DENSITY-DIVERSITY PENALTY"></a>TRAINING COMPRESSED FULLY-CONNECTED NET-WORKS WITH A DENSITY-DIVERSITY PENALTY</h3><p>用密度-多样性惩罚训练压缩的全连接网络</p>
<p>看标题就知道，文章将全连接层的密度和多样性也加入损失进行惩罚，意图使得网络变得更稀疏多样性更差。</p>
<p>这点与我们角相异惩罚的想法相反，多样性更差意味着特征表达能力更差，精度上不能接受。</p>
<h3 id="Sparse-Training-via-Boosting-Pruning-Plasticity-with-Neuroregeneration"><a href="#Sparse-Training-via-Boosting-Pruning-Plasticity-with-Neuroregeneration" class="headerlink" title="Sparse Training via Boosting Pruning Plasticity with Neuroregeneration"></a>Sparse Training via Boosting Pruning Plasticity with Neuroregeneration</h3><p>稀疏训练:促进修剪可塑性与神经再生</p>
<ol start="2021">
<li></li>
</ol>
<p>渐进修剪的方法，属于动态剪枝一类</p>
<h2 id="3、对抗剪枝"><a href="#3、对抗剪枝" class="headerlink" title="3、对抗剪枝"></a>3、对抗剪枝</h2><h3 id="Towards-Optimal-Structured-CNN-Pruning-via-Generative-Adversarial-Learning"><a href="#Towards-Optimal-Structured-CNN-Pruning-via-Generative-Adversarial-Learning" class="headerlink" title="Towards Optimal Structured CNN Pruning via Generative Adversarial Learning"></a>Towards Optimal Structured CNN Pruning via Generative Adversarial Learning</h3><p>通过生成对抗学习实现最优结构化 CNN 剪枝</p>
<p>主要思路就是把剪枝网络设为生成器，将输出特征作为Fake，再选一个泛化优异的模型做Baseline，Baseline的输出作为Real，再设计一个全连接层做判别器，最终达到低精度损失的剪枝目的。</p>
<p>这篇很晚才看到，我们前段时间正好做了个类似的实验，还觉得自己对抗剪枝的想法很新颖，做了些相关测试，后来才看到该文献，发现与前人暗合，就放弃了，实在可惜。</p>
<h2 id="4、彩票理论"><a href="#4、彩票理论" class="headerlink" title="4、彩票理论"></a>4、彩票理论</h2><h3 id="The-Lottery-Ticket-Hypothesis-Finding-Sparse-Trainable-Neural-Networks"><a href="#The-Lottery-Ticket-Hypothesis-Finding-Sparse-Trainable-Neural-Networks" class="headerlink" title="The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"></a>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</h3><p>Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.</p>
<p>彩票假设：寻找稀疏的、可训练的神经网络</p>
<p>作者的假设：密集、随机初始化的前馈网络包含子网络(中奖彩票)，当独立训练时，这些子网络在类似的迭代次数中达到与原始网络相当的测试精度。</p>
<p>彩票假说的提出，是对传统“预训练-剪枝-精度恢复”工作流的挑战。在这之前通常认为预训练模型的权重对压缩后模型的精度恢复十分重要，然而彩票理论却认为通过随机初始化权重的子网络仍可以达到原始网络的精度。</p>
<p>作者找到的中奖彩票的原始初始化彩票：它们的连接具有初始权重，这使得训练特别有效。然后提出了一个算法，以确定中奖彩票和一系列的实验，支持彩票假说和这些偶然初始化的重要性。</p>
<p>寻找中奖网络：作者提出了迭代幅值剪枝（IMP，iterative magnitude pruning）方法：首先，对初始的稠密网络进行多轮训练，直到模型收敛。然后确定最小的s%个权值，并创建一个二值掩模对这些权值进行剪枝。随后，使用之前的初始化权值重新训练稀疏化后的网络。在模型收敛后，我们重复该剪枝过程，并使用新一轮剪枝后发现的掩模设置初始化权值。我们重复这个过程，直到达到期望的稀疏程度，或者测试准确率显著下降。</p>
<p>正好我前一段时间思考了这么一个问题，我还是蛮相信缘分的。NeurlPS2020的剪枝相关论文都有提到彩票理论，剪枝邻域的探索还在继续。（好好看好好学）</p>
<hr>
<p><strong>围绕彩票假设的扩展研究：</strong></p>
<h3 id="Evaluating-lottery-tickets-under-distributional-shifts"><a href="#Evaluating-lottery-tickets-under-distributional-shifts" class="headerlink" title="Evaluating lottery tickets under distributional shifts"></a>Evaluating lottery tickets under distributional shifts</h3><p>Shrey Desai, Hongyuan Zhan, and Ahmed Aly. Evaluating lottery tickets under distributional shifts. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 153–162, 2019.</p>
<p>评估分配转移下的彩票</p>
<p>作者集中于评估分布移位下稀疏子网络的初始化，研究在源域中获得的稀疏子网络可以在不同的目标域中隔离地重新训练的程度，另外也考察了不同初始化策略在传输时的效果。最后表明，通过彩票训练获得的稀疏子网络并不是简单地过度适合于特定领域，而是反映了深层神经网络的诱导偏差，可以在多个领域中加以利用。</p>
<h3 id="Stabilizing-the-Lottery-Ticket-Hypothesis"><a href="#Stabilizing-the-Lottery-Ticket-Hypothesis" class="headerlink" title="Stabilizing the Lottery Ticket Hypothesis"></a>Stabilizing the Lottery Ticket Hypothesis</h3><p>Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis. arXiv preprint arXiv:1903.01611, 2019.</p>
<p>稳定彩票假说</p>
<p>IMP在更深层的网络中很难得到子网络，作者对其进行了改进，比如在训练早期修剪，但不是最开始。另外作者研究了子网的稳定性，提供了新见解。</p>
<h3 id="One-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers"><a href="#One-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers" class="headerlink" title="One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"></a>One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</h3><p>Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. stat, 1050:6, 2019.</p>
<p>作者通过为一种训练配置（优化器和数据集）生成中奖券并评估中奖网络在另一种配置上的性能。由足够大的数据集生成的中奖彩票初始化包含更广泛地适用于神经网络的归纳偏差，这改进了许多设置的训练，并为开发更好的初始化方法提供了希望。</p>
<h3 id="Deconstructing-Lottery-Tickets-Zeros-Signs-and-the-Supermask"><a href="#Deconstructing-Lottery-Tickets-Zeros-Signs-and-the-Supermask" class="headerlink" title="Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"></a>Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask</h3><p>Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. Deconstructing lottery tickets: Zeros, signs, and the supermask. arXiv preprint arXiv:1905.01067, 2019.</p>
<p>解构彩票：零、符号和超级面具</p>
<p>彩票网络假设的维护派，对其算法进行了分析，消除一些影响因素。另外展示了为什么要把权重置零，以及一些系列重要做法。</p>
<h3 id="Proving-the-Lottery-Ticket-Hypothesis-Pruning-is-All-You-Need"><a href="#Proving-the-Lottery-Ticket-Hypothesis-Pruning-is-All-You-Need" class="headerlink" title="Proving the Lottery Ticket Hypothesis: Pruning is All You Need"></a>Proving the Lottery Ticket Hypothesis: Pruning is All You Need</h3><p>Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. In International Conference on Machine Learning, pages 6682–6691. PMLR, 2020.</p>
<p>证明彩票假设：你需要修剪</p>
<p>作者从理论上证明了彩票假说，表明对于每个有界分布和每个具有有界权重的目标网络，具有随机权重的充分过度参数化的神经网络包含的子网络大致为与目标网络相同的精度，无需任何进一步的训练。</p>
<p>（代细读</p>
<h3 id="Slot-Machines-Discovering-Winning-Combinations-of-Random-Weights-in-Neural-Networks"><a href="#Slot-Machines-Discovering-Winning-Combinations-of-Random-Weights-in-Neural-Networks" class="headerlink" title="Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks"></a>Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks</h3><ol start="2021">
<li></li>
</ol>
<p>老虎机:发现神经网络中随机权重的获胜组合</p>
<p>作者发现仅为每个连接分配少量的随机值，不通过训练，模型也能表现出不错的性能。如果每个连接都有足够的随机权重选项，就存在这些随机权重值的选择，其泛化性能可与具有相同架构的传统训练网络相媲美。</p>
<p>这是个很有趣的现象，也进一步证明了神经网络巨大的表达能力。</p>
<p>（待深入研究）</p>
<h3 id="Efficient-Lottery-Ticket-Finding-Less-Data-is-More"><a href="#Efficient-Lottery-Ticket-Finding-Less-Data-is-More" class="headerlink" title="Efficient Lottery Ticket Finding: Less Data is More"></a>Efficient Lottery Ticket Finding: Less Data is More</h3><ol start="2021">
<li></li>
</ol>
<p>高效彩票发现：数据越少越好</p>
<p>深层网络的样本要么在训练时难以记忆，要么在修剪时很容易忘记。作者设定修剪感知临界集合（PrAC）来选取特定的样本，实现高效彩票发现。</p>
<h3 id="Rigging-the-Lottery-Making-All-Tickets-Winners"><a href="#Rigging-the-Lottery-Making-All-Tickets-Winners" class="headerlink" title="Rigging the Lottery: Making All Tickets Winners"></a>Rigging the Lottery: Making All Tickets Winners</h3><p>操纵彩票：让所有彩票中奖</p>
<p>动态剪枝的方法，算法分四个部分：稀疏分布，更新计划，删除标准，增长标准。</p>
<p>具体来说：余弦衰减更新稀疏率，删除小权重，添加最大梯度的权重，循环。</p>
<p>作者也尝试了选择梯度方向最小值来激活连接，但没有取得好效果。读到这里时我是比较无语的，因为我现在做的骨架填充和置换就是这个思路，就摁卷吧。</p>
<h3 id="Do-We-Actually-Need-Dense-Over-Parameterization-In-Time-Over-Parameterization-in-Sparse-Training"><a href="#Do-We-Actually-Need-Dense-Over-Parameterization-In-Time-Over-Parameterization-in-Sparse-Training" class="headerlink" title="Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training"></a>Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training</h3><p>我们真的需要稠密的参数化吗？稀疏训练时间序列上的过参数</p>
<p>提出的时间序列过参数很有新意，虽然就是给动态剪枝换个概念。文中大量的实验和解释，各种因素的影响。</p>
<h3 id="Connectivity-Matters-Neural-Network-Pruning-Through-the-Lens-of-Effective-Sparsity"><a href="#Connectivity-Matters-Neural-Network-Pruning-Through-the-Lens-of-Effective-Sparsity" class="headerlink" title="Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity"></a>Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity</h3><p>连通性问题：通过有效稀疏性的透镜修剪神经网络</p>
<p>从有效压缩入手（有效稀疏度，一般稀疏度），即我们做的核链（服了）。主要考虑层比例，不像SNIP,GraSP一样的层分布。</p>
<p>后面的两个做法不是很明白（二分法确定压缩率？），有时间复现下再补充。</p>
<hr>
<p><strong>质疑彩票假设：</strong></p>
<h3 id="Rethinking-the-Value-of-Network-Pruning"><a href="#Rethinking-the-Value-of-Network-Pruning" class="headerlink" title="Rethinking the Value of Network Pruning"></a>Rethinking the Value of Network Pruning</h3><p>Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. In International Conference on Learning Representations, 2018.</p>
<p>重新思考网络剪枝的价值</p>
<p>作者检查了六种先进的剪枝算法，得到的结果：1）训练一个大的、过度参数化的模型不是获得有效的最终模型所必需的，2）大模型的学习“重要”权重不一定对小剪枝模型有用，3）修剪后的架构本身，而不是一组继承的“重要”权重，是导致最终模型效率提高的原因，这表明某些修剪算法可以被视为执行网络架构搜索。</p>
<h3 id="The-State-of-Sparsity-in-Deep-Neural-Networks"><a href="#The-State-of-Sparsity-in-Deep-Neural-Networks" class="headerlink" title="The State of Sparsity in Deep Neural Networks"></a>The State of Sparsity in Deep Neural Networks</h3><p>Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.</p>
<p>深度神经网络中的稀疏状态</p>
<p>作者大规模复制了彩票假设的实验，并表明通过剪枝学习的非结构化稀疏架构无法从头开始训练到与训练模型相同的测试集性能联合稀疏化和优化。</p>
<p>对此作者很生气😡，开源了代码，并给出模型最好的状态，以便未来压缩和稀疏化工作建立严格的基线。</p>
<p>（代细读）</p>
<h3 id="Comparing-Rewinding-and-Fine-tuning-in-Neural-Network-Pruning"><a href="#Comparing-Rewinding-and-Fine-tuning-in-Neural-Network-Pruning" class="headerlink" title="Comparing Rewinding and Fine-tuning in Neural Network Pruning"></a>Comparing Rewinding and Fine-tuning in Neural Network Pruning</h3><p>Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural network pruning. arXiv preprint arXiv:2003.02389, 2020.</p>
<p>比较神经网络修剪中的回绕和微调</p>
<p>《比较》</p>
<p><strong>&#x3D;&gt; 初始化时修剪：</strong></p>
<h3 id="SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY"><a href="#SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY" class="headerlink" title="SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY"></a>SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY</h3><p>Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. International Conference on Learning Representations, ICLR, 2019.</p>
<p>基于连接敏感度的单次网络剪枝</p>
<p>19年的一篇会议论文，作者提到现有的方法破坏了网络的效用，不能保持原网络的性能，而且剪枝再训练的过程极其繁琐。他们设计了一个标准，以数据相关的方式直接衡量连接的重要性，减少了对权值的依赖，一开始就对网络进行一次剪枝，然后在稀疏剪枝后的网络上进行训练。</p>
<p>具体做法就是定义一个辅助矩阵来做剪枝标记，根据剪掉和保留导致的损失值变化来确定其网络的连接，按照需求的剪枝率保留损失值变化大的，最后近似成把导数大小（无论符号）作为对损失的影响，所以根据导数来设定权重分数进行修剪。</p>
<p>一些细节问题：</p>
<ul>
<li><p>初始化权重对损失值变动影响很大，所以初始化的时候要注意梯度在合理的范围，整个网络方差保持不变具有更好鲁棒性。</p>
</li>
<li><p>损失值变化与数据集关系也很大，作者说选取小批量数据就可</p>
</li>
</ul>
<h3 id="PICKING-WINNING-TICKETS-BEFORE-TRAINING-BY-PRESERVING-GRADIENT-FLOW"><a href="#PICKING-WINNING-TICKETS-BEFORE-TRAINING-BY-PRESERVING-GRADIENT-FLOW" class="headerlink" title="PICKING WINNING TICKETS BEFORE TRAINING BY PRESERVING GRADIENT FLOW"></a>PICKING WINNING TICKETS BEFORE TRAINING BY PRESERVING GRADIENT FLOW</h3><p>Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In International Conference on Learning Representations, 2019.</p>
<p>通过保持梯度流在训练前挑选中奖彩票</p>
<p>作者定义一个称之为梯度信号保持的剪枝标准，是基于SNIP和彩票假设的一种扩展，SNIP只考虑一个权重的梯度，容易删除一些重要的连接，彩票假设需要重复的训练。</p>
<p>做法是将梯度再进行一次微分，相当于根据损失值的变化率来评估，称保持梯度流，用泰勒近似来求解的，公式这一块没有SNIP清晰（简单）。</p>
<p>虽然GraSP一定程度上保证了高压缩率下的性能，但比不过迭代考虑层连接的剪枝方案，另外对梯度范数敏感处理GraSP也存在问题，这点在我的论文中有所改进。</p>
<h3 id="Pruning-via-Iterative-Ranking-of-Sensitivity-Statistics"><a href="#Pruning-via-Iterative-Ranking-of-Sensitivity-Statistics" class="headerlink" title="Pruning via Iterative Ranking of Sensitivity Statistics"></a>Pruning via Iterative Ranking of Sensitivity Statistics</h3><p>Stijn Verdenius, Maarten Stol, and Patrick Forré. Pruning via iterative ranking of sensitivity statistics. arXiv preprint arXiv:2006.00896, 2020.</p>
<p>通过敏感性统计的迭代排序进行修剪</p>
<p>SNIP的迭代应用。</p>
<h3 id="ESPN-Extremely-Sparse-Pruned-Networks"><a href="#ESPN-Extremely-Sparse-Pruned-Networks" class="headerlink" title="ESPN: Extremely Sparse Pruned Networks"></a>ESPN: Extremely Sparse Pruned Networks</h3><p>Minsu Cho, Ameya Joshi, and Chinmay Hegde. Espn: Extremely sparse pruned networks. arXiv preprint arXiv:2006.15741, 2020.</p>
<p>ESPN：非常稀疏的修剪网络</p>
<p>单次网络修剪方法（如 SNIP）与彩票类型方法之间的混合方法</p>
<p>论文中的实验数据很牛逼，代研究。</p>
<h3 id="PROGRESSIVE-SKELETONIZATION-TRIMMING-MORE-FAT-FROM-A-NETWORK-AT-INITIALIZATION"><a href="#PROGRESSIVE-SKELETONIZATION-TRIMMING-MORE-FAT-FROM-A-NETWORK-AT-INITIALIZATION" class="headerlink" title="PROGRESSIVE SKELETONIZATION: TRIMMING MORE FAT FROM A NETWORK AT INITIALIZATION"></a>PROGRESSIVE SKELETONIZATION: TRIMMING MORE FAT FROM A NETWORK AT INITIALIZATION</h3><p>Pau de Jorge, Amartya Sanyal, Harkirat S Behl, Philip HS Torr, Gregory Rogez, and Puneet K Dokania. Progressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint arXiv:2006.09081, 2020.</p>
<p>渐进式骨骼化：初始化时从网络中修剪更多冗余</p>
<p>基于SNIP的扩展，作者表明稀疏程度超过95%的网络很难保持性能，甚至比随机修剪的网络还差，提出骨架化网络（网络骨架的意思）。一方面迭代SNIP，允许前期不重要的权重在后期变得重要；另一方面允许已经删减的权重能够恢复。</p>
<p>目前来说，迭代过的方案都违背了训练前修剪的初衷。</p>
<hr>
<p><strong>&#x3D;&gt; 更深的理论研究：</strong></p>
<h3 id="A-Signal-Propagation-Perspective-for-Pruning-Neural-Networks-at-Initialization"><a href="#A-Signal-Propagation-Perspective-for-Pruning-Neural-Networks-at-Initialization" class="headerlink" title="A Signal Propagation Perspective for Pruning Neural Networks at Initialization"></a>A Signal Propagation Perspective for Pruning Neural Networks at Initialization</h3><p>Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip HS Torr. A signal propagation perspective for pruning neural networks at initialization. In International Conference on Learning Representations, 2019.</p>
<p>基于信号传播的神经网络初始化剪枝方法</p>
<p>为什么修剪一个未经训练的、随机初始化的神经网络是有效的还不清楚。作者通过将连接敏感度作为梯度的形式，正式描述了初始化条件，以确保可靠的连接敏感度测量，从而产生有效的修剪结果。</p>
<p>作者证明了保证连通敏感度的充分条件是分层动态等距。修剪网络会破坏动态等距，所以提出了一种无数据方法来恢复给定稀疏拓扑的分层正交性。</p>
<p>等等（代细读）。</p>
<h3 id="Pruning-neural-networks-without-any-data-by-iteratively-conserving-synaptic-flow"><a href="#Pruning-neural-networks-without-any-data-by-iteratively-conserving-synaptic-flow" class="headerlink" title="Pruning neural networks without any data by iteratively conserving synaptic flow"></a>Pruning neural networks without any data by iteratively conserving synaptic flow</h3><p>Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in Neural Information Processing Systems, 33, 2020.</p>
<p>通过迭代保存突触流在没有任何数据的情况下修剪神经网络</p>
<p>作者提出找到稀疏网络不需要训练，或者不需要查看数据，他们从数学上推导并通过实验验证了一个守恒定律，解释了现有的基于梯度修剪算法为什么初始化时会导致层崩溃，该理论还阐明了如何完全避免层崩溃，从而激发了一种新的修剪算法――迭代突触流修剪（SynFlow）。</p>
<ul>
<li><p>文中的两个假设：剪枝的临界压缩等于网络的最大压缩；层大小和平均层分数之间成反比。</p>
</li>
<li><p>突触显著性(Synaptic saliency)：一类分数指标，梯度与参数的阿达玛积。$\mathcal{S}(\theta)&#x3D;\frac{\partial \mathcal{R}}{\partial \theta} \odot \theta$</p>
</li>
<li><p>文中证明出的定理：</p>
<ul>
<li><p>突触显著性的神经元守恒 $\left(\mathcal{S}^{i n}&#x3D;\left\langle\frac{\partial \mathcal{R}}{\partial \theta^{i n}}, \theta^{i n}\right\rangle\right) &#x3D; \left(\mathcal{S}^{\text {out }}&#x3D;\left\langle\frac{\partial \mathcal{R}}{\partial \theta^{\text {out }}}, \theta^{\text {out }}\right\rangle\right) .$</p>
</li>
<li><p>突触显著性的网络守恒</p>
</li>
<li><p>迭代、积极和保守的评分实现了最大的临界压缩</p>
</li>
</ul>
</li>
<li><p>守恒和迭代修剪可避免层崩溃</p>
</li>
<li><p>重新定义一个与数据无关的损失函数</p>
</li>
</ul>
<h3 id="Logarithmic-Pruning-is-All-Y-ou-Need"><a href="#Logarithmic-Pruning-is-All-Y-ou-Need" class="headerlink" title="Logarithmic Pruning is All Y ou Need"></a>Logarithmic Pruning is All Y ou Need</h3><p>Orseau L, Hutter M, Rivasplata O. Logarithmic Pruning is All You Need. 2020.</p>
<p>你需要对数修剪</p>
<p>自彩票假说之后提出了很多假说，本文否定了很多假设，提供了更严格的界限：超参数化网络只需要每个目标子网络权重的对数因子神经元数（除深度之外的所有变量）。前面的意思就是可以得到一个对数因子来确定剪枝网络的参数量，作者进行了很多证明和推导，不过我没整明白。</p>
<h3 id="ROBUST-PRUNING-AT-INITIALIZATION"><a href="#ROBUST-PRUNING-AT-INITIALIZATION" class="headerlink" title="ROBUST PRUNING AT INITIALIZATION"></a>ROBUST PRUNING AT INITIALIZATION</h3><p>Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, and Yee Whye Teh. Robust pruning at initialization.</p>
<p>初始化时的健壮修剪</p>
<p>作者使用原则性的缩放和重新参数化来解决极限修剪时的层崩塌问题，其中提出的EOC初始化方法有待考究，原文相当的长，附录中有大量的推导，对于我来说还是太早了。</p>
<h3 id="Finding-trainable-sparse-networks-through-Neural-Tangent-Transfer"><a href="#Finding-trainable-sparse-networks-through-Neural-Tangent-Transfer" class="headerlink" title="Finding trainable sparse networks through Neural Tangent Transfer"></a>Finding trainable sparse networks through Neural Tangent Transfer</h3><p>Tianlin Liu and Friedemann Zenke. Finding trainable sparse networks through neural tangent transfer. In International Conference on Machine Learning, 2020.</p>
<p>作者介绍了神经切线转移，这是一种以无标签方式寻找可训练稀疏网络的方法。依据稀疏网络的训练动态，以神经切线核为特征，模仿函数空间中密集网络的训练动态。</p>
<p>（代细读）</p>
<hr>
<p><strong>&#x3D;&gt; 单次修剪的质疑：</strong></p>
<h3 id="PRUNING-NEURAL-NETWORKS-AT-INITIALIZATION-WHY-ARE-WE-MISSING-THE-MARK"><a href="#PRUNING-NEURAL-NETWORKS-AT-INITIALIZATION-WHY-ARE-WE-MISSING-THE-MARK" class="headerlink" title="PRUNING NEURAL NETWORKS AT INITIALIZATION: WHY ARE WE MISSING THE MARK?"></a>PRUNING NEURAL NETWORKS AT INITIALIZATION: WHY ARE WE MISSING THE MARK?</h3><p>Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? arXiv preprint arXiv:2009.08576, 2020.</p>
<p>在初始化时修剪神经网络：为什么我们忽略了掩码？</p>
<p>作者质疑了SNIP，GraSP和SynFlow，初始化删减是否对性能有限制，并评估了这些方法在初始化时的有效性。</p>
<p>（代细读）</p>
<hr>
<h2 id="、网络压缩相关（剪枝之后的进一步压缩方法）"><a href="#、网络压缩相关（剪枝之后的进一步压缩方法）" class="headerlink" title="*、网络压缩相关（剪枝之后的进一步压缩方法）"></a>*、网络压缩相关（剪枝之后的进一步压缩方法）</h2><h3 id="Binarized-neural-networks-Training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1"><a href="#Binarized-neural-networks-Training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1" class="headerlink" title="Binarized neural networks Training deep neural networks with weights and activations constrained to+ 1 or-1"></a>Binarized neural networks Training deep neural networks with weights and activations constrained to+ 1 or-1</h3><p>二值化神经网络：训练神经网络的权值和激活约束为+1或−1</p>
<p>这是2016年的一篇关于模型量化的文章，在网络量化中，有一种极为诱惑的量化方式：1bit量化——极致的压缩！但是1bit的网络在训练时有诸多细节需要完善。</p>
<p>通过简单的二值化函数，前向传播时权重会与一个权重系数相乘，反向传播时先更新之后再量化。</p>
<p>其中BN操作和其他优化的乘法可以用位移操作来代替，加快运算速度。</p>
<h3 id="DEEP-COMPRESSION-COMPRESSING-DEEP-NEURAL-NETWORKS-WITH-PRUNING-TRAINED-QUANTIZATION-AND-HUFFMAN-CODING"><a href="#DEEP-COMPRESSION-COMPRESSING-DEEP-NEURAL-NETWORKS-WITH-PRUNING-TRAINED-QUANTIZATION-AND-HUFFMAN-CODING" class="headerlink" title="DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING"></a>DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</h3><p>深度压缩：通过剪枝、训练量化和霍夫曼编码对深度神经网络进行压缩</p>
<p>首先权重剪枝（小于阈值的剪掉，压缩稀疏行&#x2F;列储存），然后量化和权重共享（聚类方法），最后Huffman Coding进一步压缩。</p>
<p>通过这三阶段的压缩，直接进行一个敌的无。</p>
<h3 id="Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks"><a href="#Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks" class="headerlink" title="Learning both Weights and Connections for Efficient Neural Networks"></a>Learning both Weights and Connections for Efficient Neural Networks</h3><p>学习权重和有效的神经网络连接</p>
<p>剪枝三步走，训练、剪枝、重训练。</p>
<h3 id="Predicting-Parameters-in-Deep-Learning"><a href="#Predicting-Parameters-in-Deep-Learning" class="headerlink" title="Predicting Parameters in Deep Learning"></a>Predicting Parameters in Deep Learning</h3><p>预测深度学习中的参数</p>
<p>说的好听，其实是低秩近似的方法，over。</p>
<h3 id="Pruning-Convolutional-Neural-Networks-for-Resource-Efficient-Transfer-Learning"><a href="#Pruning-Convolutional-Neural-Networks-for-Resource-Efficient-Transfer-Learning" class="headerlink" title="Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning"></a>Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning</h3><p>剪枝卷积神经网络在资源高效传输中的应用</p>
<p>论文中提出了一个基于泰勒展开的新准则，用它去近似由于修剪网络参数引起的损失函数的变化。</p>
<p>文中有不少巧妙的细节，值得学习，保留。</p>
<h2 id="、—"><a href="#、—" class="headerlink" title="#、—"></a>#、—</h2></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="link-muted mr-2" rel="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%89%AA%E6%9E%9D/">神经网络剪枝</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/06/28/22/clg9hsdfs000exkikezlm0cdb/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">hello-myblog</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/01/27/00/clg9hsdga0011xkikc8hh1rga/"><span class="level-item">强化学习（Reinforcement Learning）</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="香槟酒气满天飞"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">香槟酒气满天飞</p><p class="is-size-6 is-block">有目标的人才迷路，我是来人间散步的</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>地球，银河系，C137宇宙</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kangxiatao" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/kangxiatao"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="哔哩哔哩" href="https://space.bilibili.com/26509662"><i class="fab fa-youtube"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="豆瓣" href="https://www.douban.com/people/anoi/"><i class="fab fa-imdb"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#后面看的论文都懒得更新了"><span class="level-left"><span class="level-item">后面看的论文都懒得更新了</span></span></a></li><li><a class="level is-mobile" href="#0、my-codes"><span class="level-left"><span class="level-item">0、my codes</span></span></a></li><li><a class="level is-mobile" href="#1、筛选修剪"><span class="level-left"><span class="level-item">1、筛选修剪</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#AutoPrune-Automatic-Network-Pruning-by-Regularizing-Auxiliary-Parameters"><span class="level-left"><span class="level-item">AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters</span></span></a></li><li><a class="level is-mobile" href="#A-dynamic-CNN-pruning-method-based-on-matrix-similarity"><span class="level-left"><span class="level-item">A dynamic CNN pruning method based on matrix similarity</span></span></a></li><li><a class="level is-mobile" href="#Channel-Pruning-for-Accelerating-Very-Deep-Neural-Networks"><span class="level-left"><span class="level-item">Channel Pruning for Accelerating Very Deep Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#Channel-level-Acceleration-of-Deep-Face-Representations"><span class="level-left"><span class="level-item">Channel-level Acceleration of Deep Face Representations</span></span></a></li><li><a class="level is-mobile" href="#Dynamic-Channel-Pruning-Feature-Boosting-and-Suppression"><span class="level-left"><span class="level-item">Dynamic Channel Pruning: Feature Boosting and Suppression</span></span></a></li><li><a class="level is-mobile" href="#Dynamic-Network-Surgery-for-Efficient-DNNs"><span class="level-left"><span class="level-item">Dynamic Network Surgery for Efficient DNNs</span></span></a></li><li><a class="level is-mobile" href="#Efficient-Hardware-Realization-of-Convolutional-Neural-Networks-using-Intra-Kernel-Regular-Pruning"><span class="level-left"><span class="level-item">Efficient Hardware Realization of Convolutional Neural Networks using Intra-Kernel Regular Pruning</span></span></a></li><li><a class="level is-mobile" href="#Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration"><span class="level-left"><span class="level-item">Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration</span></span></a></li><li><a class="level is-mobile" href="#Infinite-Feature-Selection"><span class="level-left"><span class="level-item">Infinite Feature Selection</span></span></a></li><li><a class="level is-mobile" href="#Learning-to-Prune-Filters-in-Convolutional-Neural-Networks"><span class="level-left"><span class="level-item">Learning to Prune Filters in Convolutional Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#NISP-Pruning-Networks-using-Neuron-Importance-Score-Propagation"><span class="level-left"><span class="level-item">NISP: Pruning Networks using Neuron Importance Score Propagation</span></span></a></li><li><a class="level is-mobile" href="#Optimal-Brain-Damage"><span class="level-left"><span class="level-item">Optimal Brain Damage</span></span></a></li><li><a class="level is-mobile" href="#Optimal-Brain-Surgeon-and-General-Network-Prunng"><span class="level-left"><span class="level-item">Optimal Brain Surgeon and General Network Prunng</span></span></a></li><li><a class="level is-mobile" href="#Pruning-Filter-in-Filter"><span class="level-left"><span class="level-item">Pruning Filter in Filter</span></span></a></li><li><a class="level is-mobile" href="#Rethinking-the-Smaller-Norm-Less-Informative-Assumption-in-Channel-Pruning-of-Convolution-Layers"><span class="level-left"><span class="level-item">Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers</span></span></a></li><li><a class="level is-mobile" href="#Strategies-for-Re-training-a-Pruned-Neural-Network-in-an-Edge-Computing-Paradigm"><span class="level-left"><span class="level-item">Strategies for Re-training a Pruned Neural Network in an Edge Computing Paradigm</span></span></a></li><li><a class="level is-mobile" href="#Soft-filter-pruning-for-accelerating-deep-convolutional-neural-networks"><span class="level-left"><span class="level-item">Soft filter pruning for accelerating deep convolutional neural networks</span></span></a></li><li><a class="level is-mobile" href="#The-Generalization-Stability-Tradeoff-In-Neural-Network-Pruning"><span class="level-left"><span class="level-item">The Generalization-Stability Tradeoff In Neural Network Pruning</span></span></a></li><li><a class="level is-mobile" href="#Parameter-Efficient-Training-of-Deep-Convolutional-Neural-Networks-by-Dynamic-Sparse-Reparameterization"><span class="level-left"><span class="level-item">Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization</span></span></a></li><li><a class="level is-mobile" href="#MLPrune-Multi-Layer-Pruning-for-Automated-Neural-Network-Compression"><span class="level-left"><span class="level-item">MLPrune: Multi-Layer Pruning for Automated Neural Network Compression</span></span></a></li><li><a class="level is-mobile" href="#Exploiting-Kernel-Sparsity-and-Entropy-for-Interpretable-CNN-Compression"><span class="level-left"><span class="level-item">Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression</span></span></a></li><li><a class="level is-mobile" href="#Group-Fisher-Pruning-for-Practical-Network-Compression"><span class="level-left"><span class="level-item">Group Fisher Pruning for Practical Network Compression</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2、稀疏惩罚"><span class="level-left"><span class="level-item">2、稀疏惩罚</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Fast-ConvNets-Using-Group-wise-Brain-Damage"><span class="level-left"><span class="level-item">Fast ConvNets Using Group-wise Brain Damage</span></span></a></li><li><a class="level is-mobile" href="#LEARNING-SPARSE-NEURAL-NETWORKS-THROUGH-L0-REGULARIZATION"><span class="level-left"><span class="level-item">LEARNING SPARSE NEURAL NETWORKS THROUGH L0 REGULARIZATION</span></span></a></li><li><a class="level is-mobile" href="#Learning-the-Number-of-Neurons-in-Deep-Networks"><span class="level-left"><span class="level-item">Learning the Number of Neurons in Deep Networks</span></span></a></li><li><a class="level is-mobile" href="#Learning-Structured-Sparsity-in-Deep-Neural-Networks"><span class="level-left"><span class="level-item">Learning Structured Sparsity in Deep Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#Learning-Efficient-Convolutional-Networks-through-Network-Slimming"><span class="level-left"><span class="level-item">Learning Efficient Convolutional Networks through Network Slimming</span></span></a></li><li><a class="level is-mobile" href="#Pruning-Filters-for-Efficient-ConvNets"><span class="level-left"><span class="level-item">Pruning Filters for Efficient ConvNets</span></span></a></li><li><a class="level is-mobile" href="#PRUNE-YOUR-NEURONS-BLINDLY-NEURAL-NETWORK-COMPRESSION-THROUGH-STRUCTURED-CLASS-BLIND-PRUNING"><span class="level-left"><span class="level-item">PRUNE YOUR NEURONS BLINDLY: NEURAL NETWORK COMPRESSION THROUGH STRUCTURED CLASS-BLIND PRUNING</span></span></a></li><li><a class="level is-mobile" href="#Sparse-Convolutional-Neural-Networks"><span class="level-left"><span class="level-item">Sparse Convolutional Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#TRAINING-COMPRESSED-FULLY-CONNECTED-NET-WORKS-WITH-A-DENSITY-DIVERSITY-PENALTY"><span class="level-left"><span class="level-item">TRAINING COMPRESSED FULLY-CONNECTED NET-WORKS WITH A DENSITY-DIVERSITY PENALTY</span></span></a></li><li><a class="level is-mobile" href="#Sparse-Training-via-Boosting-Pruning-Plasticity-with-Neuroregeneration"><span class="level-left"><span class="level-item">Sparse Training via Boosting Pruning Plasticity with Neuroregeneration</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3、对抗剪枝"><span class="level-left"><span class="level-item">3、对抗剪枝</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Towards-Optimal-Structured-CNN-Pruning-via-Generative-Adversarial-Learning"><span class="level-left"><span class="level-item">Towards Optimal Structured CNN Pruning via Generative Adversarial Learning</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4、彩票理论"><span class="level-left"><span class="level-item">4、彩票理论</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#The-Lottery-Ticket-Hypothesis-Finding-Sparse-Trainable-Neural-Networks"><span class="level-left"><span class="level-item">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#Evaluating-lottery-tickets-under-distributional-shifts"><span class="level-left"><span class="level-item">Evaluating lottery tickets under distributional shifts</span></span></a></li><li><a class="level is-mobile" href="#Stabilizing-the-Lottery-Ticket-Hypothesis"><span class="level-left"><span class="level-item">Stabilizing the Lottery Ticket Hypothesis</span></span></a></li><li><a class="level is-mobile" href="#One-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers"><span class="level-left"><span class="level-item">One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</span></span></a></li><li><a class="level is-mobile" href="#Deconstructing-Lottery-Tickets-Zeros-Signs-and-the-Supermask"><span class="level-left"><span class="level-item">Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask</span></span></a></li><li><a class="level is-mobile" href="#Proving-the-Lottery-Ticket-Hypothesis-Pruning-is-All-You-Need"><span class="level-left"><span class="level-item">Proving the Lottery Ticket Hypothesis: Pruning is All You Need</span></span></a></li><li><a class="level is-mobile" href="#Slot-Machines-Discovering-Winning-Combinations-of-Random-Weights-in-Neural-Networks"><span class="level-left"><span class="level-item">Slot Machines: Discovering Winning Combinations of Random Weights in Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#Efficient-Lottery-Ticket-Finding-Less-Data-is-More"><span class="level-left"><span class="level-item">Efficient Lottery Ticket Finding: Less Data is More</span></span></a></li><li><a class="level is-mobile" href="#Rigging-the-Lottery-Making-All-Tickets-Winners"><span class="level-left"><span class="level-item">Rigging the Lottery: Making All Tickets Winners</span></span></a></li><li><a class="level is-mobile" href="#Do-We-Actually-Need-Dense-Over-Parameterization-In-Time-Over-Parameterization-in-Sparse-Training"><span class="level-left"><span class="level-item">Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training</span></span></a></li><li><a class="level is-mobile" href="#Connectivity-Matters-Neural-Network-Pruning-Through-the-Lens-of-Effective-Sparsity"><span class="level-left"><span class="level-item">Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity</span></span></a></li><li><a class="level is-mobile" href="#Rethinking-the-Value-of-Network-Pruning"><span class="level-left"><span class="level-item">Rethinking the Value of Network Pruning</span></span></a></li><li><a class="level is-mobile" href="#The-State-of-Sparsity-in-Deep-Neural-Networks"><span class="level-left"><span class="level-item">The State of Sparsity in Deep Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#Comparing-Rewinding-and-Fine-tuning-in-Neural-Network-Pruning"><span class="level-left"><span class="level-item">Comparing Rewinding and Fine-tuning in Neural Network Pruning</span></span></a></li><li><a class="level is-mobile" href="#SNIP-SINGLE-SHOT-NETWORK-PRUNING-BASED-ON-CONNECTION-SENSITIVITY"><span class="level-left"><span class="level-item">SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY</span></span></a></li><li><a class="level is-mobile" href="#PICKING-WINNING-TICKETS-BEFORE-TRAINING-BY-PRESERVING-GRADIENT-FLOW"><span class="level-left"><span class="level-item">PICKING WINNING TICKETS BEFORE TRAINING BY PRESERVING GRADIENT FLOW</span></span></a></li><li><a class="level is-mobile" href="#Pruning-via-Iterative-Ranking-of-Sensitivity-Statistics"><span class="level-left"><span class="level-item">Pruning via Iterative Ranking of Sensitivity Statistics</span></span></a></li><li><a class="level is-mobile" href="#ESPN-Extremely-Sparse-Pruned-Networks"><span class="level-left"><span class="level-item">ESPN: Extremely Sparse Pruned Networks</span></span></a></li><li><a class="level is-mobile" href="#PROGRESSIVE-SKELETONIZATION-TRIMMING-MORE-FAT-FROM-A-NETWORK-AT-INITIALIZATION"><span class="level-left"><span class="level-item">PROGRESSIVE SKELETONIZATION: TRIMMING MORE FAT FROM A NETWORK AT INITIALIZATION</span></span></a></li><li><a class="level is-mobile" href="#A-Signal-Propagation-Perspective-for-Pruning-Neural-Networks-at-Initialization"><span class="level-left"><span class="level-item">A Signal Propagation Perspective for Pruning Neural Networks at Initialization</span></span></a></li><li><a class="level is-mobile" href="#Pruning-neural-networks-without-any-data-by-iteratively-conserving-synaptic-flow"><span class="level-left"><span class="level-item">Pruning neural networks without any data by iteratively conserving synaptic flow</span></span></a></li><li><a class="level is-mobile" href="#Logarithmic-Pruning-is-All-Y-ou-Need"><span class="level-left"><span class="level-item">Logarithmic Pruning is All Y ou Need</span></span></a></li><li><a class="level is-mobile" href="#ROBUST-PRUNING-AT-INITIALIZATION"><span class="level-left"><span class="level-item">ROBUST PRUNING AT INITIALIZATION</span></span></a></li><li><a class="level is-mobile" href="#Finding-trainable-sparse-networks-through-Neural-Tangent-Transfer"><span class="level-left"><span class="level-item">Finding trainable sparse networks through Neural Tangent Transfer</span></span></a></li><li><a class="level is-mobile" href="#PRUNING-NEURAL-NETWORKS-AT-INITIALIZATION-WHY-ARE-WE-MISSING-THE-MARK"><span class="level-left"><span class="level-item">PRUNING NEURAL NETWORKS AT INITIALIZATION: WHY ARE WE MISSING THE MARK?</span></span></a></li></ul></li><li><a class="level is-mobile" href="#、网络压缩相关（剪枝之后的进一步压缩方法）"><span class="level-left"><span class="level-item">*、网络压缩相关（剪枝之后的进一步压缩方法）</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Binarized-neural-networks-Training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1"><span class="level-left"><span class="level-item">Binarized neural networks Training deep neural networks with weights and activations constrained to+ 1 or-1</span></span></a></li><li><a class="level is-mobile" href="#DEEP-COMPRESSION-COMPRESSING-DEEP-NEURAL-NETWORKS-WITH-PRUNING-TRAINED-QUANTIZATION-AND-HUFFMAN-CODING"><span class="level-left"><span class="level-item">DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</span></span></a></li><li><a class="level is-mobile" href="#Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks"><span class="level-left"><span class="level-item">Learning both Weights and Connections for Efficient Neural Networks</span></span></a></li><li><a class="level is-mobile" href="#Predicting-Parameters-in-Deep-Learning"><span class="level-left"><span class="level-item">Predicting Parameters in Deep Learning</span></span></a></li><li><a class="level is-mobile" href="#Pruning-Convolutional-Neural-Networks-for-Resource-Efficient-Transfer-Learning"><span class="level-left"><span class="level-item">Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning</span></span></a></li></ul></li><li><a class="level is-mobile" href="#、—"><span class="level-left"><span class="level-item">#、—</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-09T03:45:03.000Z">2023-04-09</time></p><p class="title"><a href="/2023/04/09/11/clg9hsdgk001hxkik1zqqg04x/">研究下管理体系</a></p><p class="categories"><a href="/categories/%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/">学习日志</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-04-09T01:42:46.000Z">2023-04-09</time></p><p class="title"><a href="/2023/04/09/09/clg9hsdfy000mxkik405dcznv/">一个思考，关于硅基生命</a></p><p class="categories"><a href="/categories/%E6%B5%AE%E7%94%9F%E6%97%A5%E8%AE%B0/">浮生日记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-14T05:30:00.000Z">2023-03-14</time></p><p class="title"><a href="/2023/03/14/13/clg9hsdfo0009xkik9faxcqso/">ROS与Gazebo联合仿真平台搭建</a></p><p class="categories"><a href="/categories/%E7%A0%81%E5%86%9C%E7%AC%94%E8%AE%B0/">码农笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-10T01:09:00.000Z">2023-03-10</time></p><p class="title"><a href="/2023/03/10/09/clg9hsdfw000kxkik5eko3ft1/">以武服人</a></p><p class="categories"><a href="/categories/%E6%B5%AE%E7%94%9F%E6%97%A5%E8%AE%B0/">浮生日记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-29T03:01:43.000Z">2022-09-29</time></p><p class="title"><a href="/2022/09/29/11/clg9hsdgg001cxkikfjqx0u3a/">深度学习之底层实现</a></p><p class="categories"><a href="/categories/%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/">学习日志</a></p></div></article></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div><p class="help">kangxiatao@gmail.com</p></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="AnoI" height="28"></a><p class="is-size-7"><span>&copy; 2023 Kang Xiatao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="To WeiBo" href="https://weibo.com/u/5639381621?ssl_rnd=1624881304.5599&amp;is_all=1"><i class="fab fa-weibo"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="To GitHub" href="https://github.com/kangxiatao"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>